---
title: "Machine Learning Project - Predicting activity quality"
fontsize: 5pt
date: "October 21, 2015"
output: html_document
---
```{r setenv, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(caret)

opts_chunk$set(echo=TRUE)
options(digits=3)

setwd("~/Coursera/Machine Learning")
set.seed(125)
```

# Executive Summary
In this study, we work on the data from the project [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) that comprises measures from activity monitors (as accelerometers on the belt, forearm, arm, and dumbell) of 6 participants during exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways and the activity quality of the exercise was classified in a categorical variable of 5 values.

We are interested in building a model to predict the manner in which they did the exercise using the measures of the activity monitors as predicitive variables. In this document, we will explain how we prepare the data, our model selection and cross-validation method. We will also give a confidence interval of the expected out of sample error.

# Data preparation
Before working on the model, we load the data, we split it into a training set and a test set and we select the variables to keep the most relevant ones.  

## Data loading
We load the [original data set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). We assign as NA the values *NA*, *#DIV/0!* and the missing values.
```{r load}
monitor <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!",""))
```

## Data splitting
Before working on data, we split our training set in 2 partitions, leaving 60% of the data for building and tuning our prediction model and 40% of the data for evaluating our model and estimating the out of sample error.
```{r partition}
inTrain <- createDataPartition(y=monitor$classe, p=0.6, list=FALSE)
training <- monitor[inTrain, ]
testing <- monitor[-inTrain, ]
```
The original data set has 160 variables and 19.622 observations. The training set has 11.776 observations and the test set has 7.846 observations.
```{r part_stat}
rbind("monitor"=c("variables"=dim(monitor)[2],"observations"=dim(monitor)[1]),
      "training"=c(dim(training)[2],dim(training)[1]),
      "testing"=c(dim(testing)[2],dim(testing)[1])
     )
```

## Variable selection
First we eliminate the 5 first variables which are not relevant as predictors, like X (row index), the user_name and the 3 timestamp variables.
```{r sel_1}
training <- training[,-c(1:5)]
```

Second we eliminate the variables where majority of data is missing (more than 50% of NAs).
```{r sel_2}
nbRows <- nrow(training)
naCols <- data.frame(nb=sapply(training, function(x) sum(is.na(x))))
naInd <- which(naCols/nbRows>0.5)
training <- training[,-naInd]
```

Finally, we check for Near Zero Variables. The only NZV after our 2 first cleaning steps is the variable new_window.
```{r sel_3}
nzv <- nearZeroVar(training, saveMetrics=TRUE)$nzv
training <- training[,!nzv]
names(training)[nzv]
```

The training data set is now composed of 54 variables, the class variable and 53 predictors (see the appendix **Data selection** to check out the list of these variables).
```{r train_stat}
dim(training)[2]
```
# Predictive model
During this phase of the project, we have only worked with the training data set and we did comparisons of the different models and the cross-validation methods based on the **in sample error**.

## Model selection
For model selection, several algorithms were used as trees, generalized linear model, random forest and boosting with trees. Random forest and boosting with trees were clearly the best models as the in sample error were far lower than other models. Finally Random forest was the best predictive model on the training set.

## Cross-validation
Cross-Validation is used for smoothing predictions and for giving a better estimation of the out of sample error. Random forest predictive models are built on cross-validation principle as it generates a lot of model by sampling the variables and by repeating the process by sampling the data set.  
Different cross-validation methods were used as bootstrap, k-folds and cross-validation. There were no clear evidence of the best strategy so the default method for random forest in caret was used, bootstrap with 25 repetitions.

## Random Forest Model
The predictive model is built using all the 53 variables. It looks very robust with an accuracy of **99.4%** and an in sample error of **0.6%**.
```{r RF, cache=TRUE, warning=FALSE, message=FALSE}
modelFitRF <- train(classe~.,data=training,method="rf")
modelFitRF
```

# Out of sample error prediction
To predict the out of sample error, we apply the predictive model on the test set.  
The expected accuracy is estimated at 99.5% (95% CI : [99.3%,99.6%]), which means that the out of sample error is expected at **0.5% (95% CI : [0.4%-0.7%])**. The out of sample error is very closed to the in sample error, which indicates that there is no overfitting issue in this model.
```{r pred, warning=FALSE, message=FALSE}
predictionsRF <- predict(modelFitRF,newdata=testing)
confusionMatrix(predictionsRF, testing$class)
```

# Conclusion
We have built a very robust predictive model for activity quality based on activity monitors measures using random forest with an **expected out of sample error of 0.5% (95% CI : [0.4%-0.7%])**.

# Appendix

## Data selection
```{r look}
str(training)
```



