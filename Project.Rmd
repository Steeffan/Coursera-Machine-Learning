---
title: "Predicting activity quality"
fontsize: 5pt
date: "Friday, Septempber 21, 2015"
output: html_document
---
```{r setenv, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(knitr)
library(caret)

opts_chunk$set(echo=TRUE)
options(digits=3)

setwd("~/Coursera/Machine Learning")
set.seed(125)
```

# Executive Summary
In this study, we work on the data from the project [linked phrase](http://groupware.les.inf.puc-rio.br/har) that comprises measures from activity monitors (as accelerometers on the belt, forearm, arm, and dumbell) of 6 participants during exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways and the activity quality of the exercise was classified in a categorical variable of 5 values.

We are interested in building a model to predict the manner in which they did the exercise using all the other available measures as potential predicitive varriables. In this document, we will explain how we prepare the data, our model selection and cross-validation method. We will also give a confidence interval of the expected out of sample error.

# Data preparation
## Load data
We load the input test data set. We assign as NA the values *NA*, *#DIV/0!* and the missing values.
```{r load}
monitor <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!",""))
```

## Data splitting
Before working on data, we split our training set in 2 partitions, leaving 60% of the data for building and tuning our prediction model (training dataframe) and 40% of the data for evaluating our model and estimating the out of sample error (testing dataframe).
```{r partition}
inTrain <- createDataPartition(y=monitor$classe, p=0.6, list=FALSE)
training <- monitor[inTrain, ]
testing <- monitor[-inTrain, ]
```
The original data set has 160 variables and 19.622 observations. The training set has 11.776 observations and the test set has 7.846 observations.
```{r part_stat}
kable(rbind(c("set","Observations","Variables"),
            c("monitor",dim(monitor)),
            c("training",dim(training)),
            c("testing",dim(testing))),
      format = "latex")
```

## Variable selection
First we eliminate the variable which are indices or represent information that we can't use as predictors.
```{r sel_1}
training <- training[,-c(1:6)]
```

Second we eliminate the variables with more than 90% of NAs.
```{r sel_2}
nbRows <- nrow(training)
naCols <- data.frame(nb=sapply(training, function(x) sum(is.na(x))))
naInd <- which(naCols/nbRows>0.9)
training <- training[,-naInd]
```

Finally, we check for Near Zero Variables. But as we can see, there are no NZV after our 2 first cleaning steps.
```{r sel_3}
nzv <- nearZeroVar(training, saveMetrics=TRUE)$nzv
training <- training[,!nzv]
sum(nzv)
```

The training data set is now composed of 54 variables, the class variable and 53 predictors (see Annex **Data selection** to check out the list of these variables).
```{r train_stat}
kable(rbind(c("Observations","Variables"),
            dim(training)),
      format = "latex")
```
# Predictive model
For model selection, several algorithms were used as trees, generalized linear model, random forest and boosting with trees. Random forest and boosting with trees were clearly the best models as the **in sample errors** were far lower than other models. Finally Random forest was the best predictive model on the training set. 

## Cross-validation
Cross-Validation is used for smoothing predictions and for giving a better estimation of the out of sample error. Random forest predictive models are built on cross-validation principle as it generates a lot of model by sampling the variables and by repeating the process by sampling the data set.  
Different cross-validation methods were used as bootstrap, k-folds and cross-validation. There were no clear evidence of the best strategy so default method for random forest in caret was used, bootstrap with 25 repetitions.

## Random Forest Model
The predictive model is built using all the other variables. It looks very robust with an accuracy of **99.4%** and an in sample error of **0.6%**.
```{r RF, cache=TRUE}
modelFitRF <- train(classe~.,data=training,method="rf")
modelFitRF
```

## Out of sample error prediction
To predict the out of sample error, we apply the predictive model on the test set.  
The expected accuracy is estimated at 99.5% ([99.3%,99.6%]), which means that the out of sample error is expected at **0.5% ([0.4%-0.7%])**. The out of sample error is very closed (and surprisingly even better) to the in of sample error, which indicates that there is no overfitting issue in this model.
```{r pred}
predictionsRF <- predict(modelFitRF,newdata=testing)
confusionMatrix(predictionsRF, testing$class)
```

# Conclusion
We have built a very robust predictive model for activity quality using random forest with an **expected out of sample error of 0.5% ([0.4%-0.7%])**.

# Appendix

## Data selection
```{r look}
str(training)
```



